\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{SAR-MFCC}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \begin{verbatim}
                                   Speaker Accent Recognition Using MFCCs
                                  -----------------------------------------
\end{verbatim}

    Table of Contents{}

{{1~~}Domain Knowledge}

{{2~~}Dataset}

{{2.1~~}Source}

{{2.2~~}Problem formulation}

{{3~~}Related Work}

{{4~~}Data Exploration}

{{5~~}Data Distribution}

{{6~~}Outliers Detection}

{{6.1~~}Interquartile Range (IQR)}

{{6.2~~}Mahalanabois Distance}

{{6.3~~}Outlier Removal Tradeoff}

{{7~~}Multicollinearity Detection}

{{7.1~~}Pearson correlation}

{{7.2~~}Hieracichal clustering}

{{8~~}Dimensionality Reduction}

{{9~~}Classification}

{{9.1~~}Imbalanced Dataset Classification}

{{9.2~~}Classification Metric}

{{9.3~~}K-fold Cross-Validation}

{{9.4~~}Classification Models}

{{9.4.1~~}K-Nearest Neighbour:}

{{9.4.2~~}Support Vector Machine:}

{{9.4.3~~}Random Forest}

{{9.4.4~~}Implementation Framework}

{{9.4.5~~}Imbalanced dataset classification}

{{9.4.6~~}Imbalanced dataset classification by modifying classifiers'
cost function}

{{10~~}Dataset Balancing:}

{{10.1~~}Synthetic Minority Oversampling Technique (SMOTE)}

{{10.2~~}Adaptive Synthetic (ADASYN)}

{{10.3~~}Oversampled Dataset Classification}

{{11~~}Conclusion}

    \hypertarget{domain-knowledge}{%
\section{Domain Knowledge}\label{domain-knowledge}}

Accent recognition is classification of the speaker accent from an input
signal. Classifying accents can provide information about a speaker's
nationality and heritage, which can help identify topics more relevant
to the user, for the purposes of search results and advertisements.
Typical applications include online banking, telephone shopping, and
security applications. Typically, the input signal is represented in
frequency domain then dimensionality reduction can be performed together
with feature extraction.

A common feature extraction technique for the purpose of speech
recognition is Mel-frequency cepstral coefficients or MFCCs. MFCCs are
coefficients that collectively make up an MFC (mel-frequency cepstrum)
which is a representation of the short-term power spectrum of a sound,
based on a linear cosine transform of a log power spectrum on a
nonlinear mel scale of frequency. The main idea of MFCC is to transform
the signal from time domain to frequency domain and to map the
transformed signal in hertz onto Mel-scale due to the fact that 1 kHz is
a threshold of humans' hearing ability.

MFCCs are commonly derived as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take the absolute value of the short time Fourier transform of (a
  windowed excerpt of) a signal.
\item
  Map the powers of the spectrum obtained above onto the mel scale
\item
  Take the logs of the powers at each of the mel frequencies.
\item
  Take the discrete cosine transform of the list of mel log powers, as
  if it were a signal.
\item
  The MFCCs are the amplitudes of the resulting spectrum.
\item
  Return the first \(q\) MFCCs.
\end{enumerate}

MFCC values are not very robust in the presence of additive noise, and
so it is common to normalise their values in speech recognition systems
to lessen the influence of noise. (energy terms)

\hypertarget{dataset}{%
\section{Dataset}\label{dataset}}

\#\# Source

A total of 329 signal data were collected from the voice of 22 speakers
11 female and 11 male of accented speakers speaking English, containing
165 US voice and 164 non-US voice from 5 countries: Spain, France,
Germany, Italy, and UK.

The sound tracks have lengths of around 1 second, with a sampling rate
of 44,100 Hz, each sound track vector on the time domain has more than
30,000 entries. Because of the method used in collecting the data, there
is no background noise in any sound tracks.

The 12th lowest order melfrequency cepstral coefficients (MFCCs) of the
audio signals are used as inputs to the algorithms.

The Source of both audio files and MFCC spreadsheet available at UCI
\href{https://archive.ics.uci.edu/ml/datasets/Speaker+Accent+Recognition}{here}

\#\# Problem formulation

This accent reognition is a classification problem and the response
variable \(yi\) is givn by:

\[
y_i = \left\{
    \begin{array}\\
        0, \space \space ES\\
        1, \space \space FR\\
        2, \space \space GE\\
        3, \space \space IT\\
        4, \space \space UK\\
        5, \space \space US 
    \end{array}
\right.
\]

showing that there are 6 class labels

The design is \textbf{balanced in terms of US/NOT US accent but we want
to extend the problem to classify all accents, hence imbalanced problem}

\hypertarget{related-work}{%
\section{Related Work}\label{related-work}}

\href{https://arxiv.org/abs/1501.07866}{1} is the original paper to
analyze the speaker accent recognition dataset . Binary classification
is performed in this paper classifying audio samples into US accent or
non-US accent (the balanced case)

\href{https://ieeexplore.ieee.org/abstract/document/9259902}{2} is the
only work considers the imbalanced case only by comparing the model
performance using alternative metrics like MSE, Kappa, precision and
recall

In my work, I tackled the imbalanced case implementing the
state-of-the-art imbalaced learning algorithms

    \# Data Exploration

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}importing libs}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import dataset}
\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accent\PYZhy{}mfcc\PYZhy{}data\PYZhy{}1.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{index} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{index}
\PY{n}{columns} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{columns}
\PY{n}{values} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{values}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}dimension of dataset}
\PY{n}{df}\PY{o}{.}\PY{n}{shape} \PY{c+c1}{\PYZsh{}or values.shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(329, 13)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}peek at the data}
\PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
  language         X1        X2        X3         X4        X5         X6  \textbackslash{}
0       ES   7.071476 -6.512900  7.650800  11.150783 -7.657312  12.484021
1       ES  10.982967 -5.157445  3.952060  11.529381 -7.638047  12.136098
2       ES   7.827108 -5.477472  7.816257   9.187592 -7.172511  11.715299

          X7        X8        X9       X10       X11       X12
0 -11.709772  3.426596  1.462715 -2.812753  0.866538 -5.244274
1 -12.036247  3.491943  0.595441 -4.508811  2.332147 -6.221857
2 -13.847214  4.574075 -1.687559 -7.204041 -0.011847 -6.463144
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} list the levels for the class}
\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array(['ES', 'FR', 'GE', 'IT', 'UK', 'US'], dtype=object)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Grab your wav }
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{io} \PY{k+kn}{import} \PY{n}{wavfile}
\PY{n}{mywav} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../accent\PYZhy{}recognition\PYZhy{}mfcc\PYZhy{}\PYZhy{}1/ES\PYZhy{}M\PYZhy{}1\PYZhy{}1.wav}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{rate}\PY{p}{,} \PY{n}{data} \PY{o}{=} \PY{n}{wavfile}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{n}{mywav}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Length in time (s): }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{/}\PY{n+nb}{float}\PY{p}{(}\PY{n}{rate}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Length in time (s):  0.7169160997732427
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} pip install ipython}
\PY{k+kn}{import} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display}
\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Audio}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{data}\PY{p}{,}\PY{n}{rate}\PY{o}{=}\PY{n}{rate}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.lib.display.Audio object>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{warnings}
\PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k+kn}{import} \PY{n}{figure}
\PY{k+kn}{import} \PY{n+nn}{wave}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{display.max\PYZus{}colwidth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{spf} \PY{o}{=} \PY{n}{wave}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{../accent\PYZhy{}recognition\PYZhy{}mfcc\PYZhy{}\PYZhy{}1/ES\PYZhy{}M\PYZhy{}1\PYZhy{}1.wav}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Extract Raw Audio from Wav File}
\PY{n}{signal} \PY{o}{=} \PY{n}{spf}\PY{o}{.}\PY{n}{readframes}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{signal} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{fromstring}\PY{p}{(}\PY{n}{signal}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Int16}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plt spanish male wave sample}
\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{80}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Spanish male sample Signal Wave}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{signal}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig}\PY{p}{,} \PY{p}{(}\PY{n}{ax1}\PY{p}{,}\PY{n}{ax2}\PY{p}{)} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{}mfcc spectrogram}
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{cm}
\PY{n}{mfcc\PYZus{}data}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{swapaxes}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{0} \PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{ax1}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{mfcc\PYZus{}data}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{cm}\PY{o}{.}\PY{n}{coolwarm}\PY{p}{,} \PY{n}{origin}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{aspect}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax1}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ceptrum Indices (X1\PYZhy{}X12)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MFCCs Spectrogram}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax1}\PY{o}{.}\PY{n}{axes}\PY{o}{.}\PY{n}{yaxis}\PY{o}{.}\PY{n}{set\PYZus{}ticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}

\PY{n}{ax2}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}\PY{p}{)}
\PY{n}{ax2}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MFCCs Range over Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax2}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    As shown below, the collected data has more (US) accent data than all of
the other five countries i.e.~distribution of examples across the known
classes is biased or skewed. \textbf{The distribution is neither slight
biased nor severe imbalanced}. The majority class is about 5th times
each of the other classes.

Imbalanced classifications pose a challenge for predictive modeling as
most of the machine learning algorithms used for classification were
designed around the assumption of an equal number of examples for each
class. This results in models that have poor predictive performance for
the minority class.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Class Distribution (imbalance check)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{language}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{df}\PY{o}{.}\PY{n}{language}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count (target)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
US    165
UK     45
IT     30
FR     30
GE     30
ES     29
Name: language, dtype: int64
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{language}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{normalize}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
US    0.501520
UK    0.136778
IT    0.091185
FR    0.091185
GE    0.091185
ES    0.088146
Name: language, dtype: float64
    \end{Verbatim}

    As the plot below shows, the variety of values in the predictors
(Cardinality) is high.

Also, no missing Values in our dataset

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}checking the cardinality of the columns (count unique values in columns)}
\PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}\PY{o}{.}\PY{n}{nunique}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{o}{.}\PY{n}{barh}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{n}{b}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{which}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{major}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}666666}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{} of unique values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cardinality}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} cardin = [predictors[col].nunique() for col in predictors.columns.tolist()]}
\PY{c+c1}{\PYZsh{} cols = [col for col in predictors.columns.tolist()]}
\PY{c+c1}{\PYZsh{} d = \PYZob{}k:v for (k, v) in zip(cols,cardin)\PYZcb{}}
\PY{c+c1}{\PYZsh{} cardinal = pd.DataFrame(list(d.items()), columns=[\PYZsq{}column\PYZsq{}, \PYZsq{}cardinality\PYZsq{}])}
\PY{c+c1}{\PYZsh{} cardinal.sort\PYZus{}values(\PYZsq{}cardinality\PYZsq{},ascending=False)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{ checking missing values }
\PY{l+s+sd}{ that is data=zero in this case because The MFCCs are the amplitudes of the resulting spectrum}
\PY{l+s+sd}{  and zero is the only value doesnt make sense in this case}

\PY{l+s+sd}{  we know already from UCI page that all data are presented am just following the standard prosudres}
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{}df.columns[df.isnull().sum()\PYZgt{}0] index of columns with null}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
language    0
X1          0
X2          0
X3          0
X4          0
X5          0
X6          0
X7          0
X8          0
X9          0
X10         0
X11         0
X12         0
dtype: int64
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Statistical Summary}
\PY{n}{df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
               X1          X2          X3          X4          X5          X6  \textbackslash{}
count  329.000000  329.000000  329.000000  329.000000  329.000000  329.000000
mean     5.645227   -4.270814    2.635319    7.200207   -5.648733    9.810236
std      5.105302    3.514350    3.635323    4.310095    4.596180    3.624654
min     -6.067831  -14.972962   -6.186583   -8.844231  -15.656596   -3.528964
25\%      2.065603   -6.515764    0.137042    4.700874   -8.417684    8.643563
50\%      4.954206   -4.252512    2.029268    7.804680   -6.786670   10.379330
75\%      9.793648   -1.560250    4.178026   10.129165   -4.463308   11.784360
max     17.749851    3.570765   17.066487   16.178942    7.912809   21.445837

               X7          X8          X9         X10         X11         X12
count  329.000000  329.000000  329.000000  329.000000  329.000000  329.000000
mean    -9.408053    5.117328   -1.229432   -2.362288    2.430833   -3.980415
std      2.484117    2.650608    3.634849    5.042034    3.478467    2.985879
min    -15.365665   -2.873862  -15.510974  -11.429178  -13.664104  -13.724103
25\%    -11.120860    3.482167   -1.749082   -5.083522    1.197789   -5.941409
50\%     -9.710399    4.843103   -0.389970   -3.323147    3.169703   -4.374334
75\%     -7.989370    6.588931    0.779993   -1.506037    4.673452   -2.191658
max     -0.424033   13.846083    4.789989   16.326455    9.166066    5.259430
\end{Verbatim}
\end{tcolorbox}
        
    The stastical summary, the MFCCs numerical values lies in the window
{[}-16, 21{]} with similar and low standard deviation is between {[}2.4
5.1{]}, though \textbf{no scaling is needed} before applying
classification models

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}types of attributes}
\PY{n}{df}\PY{o}{.}\PY{n}{dtypes}
\PY{c+c1}{\PYZsh{} all float}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
language     object
X1          float64
X2          float64
X3          float64
X4          float64
X5          float64
X6          float64
X7          float64
X8          float64
X9          float64
X10         float64
X11         float64
X12         float64
dtype: object
\end{Verbatim}
\end{tcolorbox}
        
    All independent variables are numerical only to change dependant
variable ``language'' to categorial

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} change the target datatype in order to encode }
\PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{language}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{language}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\PY{n}{df}\PY{o}{.}\PY{n}{dtypes}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
language    category
X1           float64
X2           float64
X3           float64
X4           float64
X5           float64
X6           float64
X7           float64
X8           float64
X9           float64
X10          float64
X11          float64
X12          float64
dtype: object
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} text coding needed for visualize and process }
\PY{n}{language}\PY{o}{=}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{language\PYZus{}coded}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{language}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{cat}\PY{o}{.}\PY{n}{codes} \PY{c+c1}{\PYZsh{}add another col holds categories codes (initially will be integer)}
\PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{language\PYZus{}coded}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{language\PYZus{}coded}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{}(change the col datatype to category)}
\PY{n}{df}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} drop the old language col}
\PY{n}{df}\PY{o}{.}\PY{n}{dtypes}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
X1                 float64
X2                 float64
X3                 float64
X4                 float64
X5                 float64
X6                 float64
X7                 float64
X8                 float64
X9                 float64
X10                float64
X11                float64
X12                float64
language\_coded    category
dtype: object
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} list new for the class }
\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language\PYZus{}coded}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{language\PYZus{}coded}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
5    165
4     45
3     30
2     30
1     30
0     29
Name: language\_coded, dtype: int64
    \end{Verbatim}

    {[}`ES', `FR', `GE', `IT', `UK', `US'{]} = {[}0, 1, 2, 3, 4, 5{]}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{predictors}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{}df.iloc[:,0:12]}
\PY{n}{target}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{}df.iloc[:,12:13]    =\PYZsh{}target = \PYZsq{}language\PYZus{}coded\PYZsq{}}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{data-distribution}{%
\section{Data Distribution}\label{data-distribution}}

    \textbf{Many classification algorithms, assumes variables to follow a
particular distribution (specially Gaussian distribution)}. The cost of
not meeting the assumptions could be high at times.

In statistics, \textbf{D'Agostino-Pearson's} test, is a goodness-of-fit
measure of departure from normality, that is the test aims to establish
whether or not the given sample comes from a normally distributed
population. The test is based on transformations of the sample kurtosis
and skewness, and has power only against the alternatives that the
distribution is skewed and/or kurtic.

Skew is a quantification of how much a distribution is pushed left or
right, a measure of asymmetry in the distribution.

Kurtosis quantifies how much of the distribution is in the tail. It is a
simple and commonly used statistical test for normality.

According to the D'Agostino-Pearson test, the data is normally
distributed when the test statistic \(Z^2_K+Z^2_S\) has a chi-square
distribution with 2 degrees of freedom

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{  }
\PY{l+s+sd}{  stats.normaltest function tests the null hypothesis that a sample comes from a }
\PY{l+s+sd}{  normal distribution It is based on DAgostino and Pearsons test that combines }
\PY{l+s+sd}{  skew and kurtosis to produce an omnibus test of normality}
\PY{l+s+sd}{  }
\PY{l+s+sd}{  Usually, a significance level (denoted as  or alpha) of 0.05 (standard) indicates }
\PY{l+s+sd}{  that the risk of concluding the data do not follow a normal distributionwhen, }
\PY{l+s+sd}{  actually, the data do follow a normal distributionis 5\PYZpc{}.}

\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}

\PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k+kn}{import} \PY{n}{stats}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{k2}\PY{p}{,} \PY{n}{p} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{normaltest}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.05} \PY{c+c1}{\PYZsh{}1e\PYZhy{}3}
\PY{c+c1}{\PYZsh{}     print(\PYZdq{}p = \PYZob{}\PYZcb{}\PYZdq{}.format(p))}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
    \PY{k}{if} \PY{n}{p} \PY{o}{\PYZlt{}} \PY{n}{alpha}\PY{p}{:}  \PY{c+c1}{\PYZsh{} null hypothesis: x comes from a normal distribution}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The null hypothesis can be rejected}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}not normal}
    \PY{k}{else}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The null hypothesis cannot be rejected}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}normal}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
X1
The null hypothesis can be rejected
X2
The null hypothesis cannot be rejected
X3
The null hypothesis can be rejected
X4
The null hypothesis can be rejected
X5
The null hypothesis can be rejected
X6
The null hypothesis can be rejected
X7
The null hypothesis can be rejected
X8
The null hypothesis can be rejected
X9
The null hypothesis can be rejected
X10
The null hypothesis can be rejected
X11
The null hypothesis can be rejected
X12
The null hypothesis can be rejected
    \end{Verbatim}

    \textbf{Only X2 comes from normal distribution for a \(\rho\) of 5\%}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k+kn}{import} \PY{n}{norm}
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{22}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}\PY{p}{)}
\PY{k}{try}\PY{p}{:}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{col} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{predictors}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{to\PYZus{}list}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{sns}\PY{o}{.}\PY{n}{distplot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{fit}\PY{o}{=}\PY{n}{norm}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{col}\PY{p}{)}        
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{col}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{k}{except} \PY{n+ne}{Exception} \PY{k}{as} \PY{n}{e}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{col}\PY{p}{,}\PY{n}{e}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    the figure above shows the plots of the predictors distribution with a
histogram and maximum likelihood gaussian distribution fit.

\begin{itemize}
\tightlist
\item
  Only X2 comes close to a normal distribution (confirms the
  D'Agostino-Pearson's test results)
\item
  X3,X5,X9,X10,X11 have highly skewed distribution, and X4,X6 with
  moderate Skew distribution
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{predictors}\PY{o}{.}\PY{n}{skew}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
X1     0.192326
X2    -0.226634
X3     1.131745
X4    -0.743209
X5     1.180070
X6    -0.985631
X7     0.483211
X8     0.394249
X9    -1.637664
X10    1.359030
X11   -1.686876
X12    0.321322
dtype: float64
    \end{Verbatim}

    \hypertarget{outliers-detection}{%
\section{Outliers Detection}\label{outliers-detection}}

    Outliers are innocent until proven guilty. With that being said, they
should not be removed unless there is a good reason for that. For
example, one can notice some weird, suspicious values that are unlikely
to happen, and so decides to remove them. Though, they worth
investigating before removing. It is also worth mentioning that some
models, like Support Vector Machine, are very sensitive to outliers. In
other words, outliers might throw the model off from where most of the
data lie.

we will examine two outlier detection techniques and later will compare
generated noise-free dataset split against other splits

\#\# Interquartile Range (IQR)

A boxplot can be used to indicate explicitly the presence of outliers.
Many inferential procedures are based on the assumption that the
population distribution is normal. Even a single extreme outlier/several
mild outliers in the sample warns the investigator that such procedures
may be unreliable

In the Interquartile Range (IQR) technique, Any observation farther than
1.5fs from the closest fourth is an outlier. An outlier is extreme if it
is more than 3fs from the nearest fourth, and it is mild otherwise.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot 1.5*IQR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}

\PY{n}{ax}\PY{o}{=}\PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{predictors}\PY{p}{)}
\PY{n}{ax}\PY{o}{=}\PY{n}{sns}\PY{o}{.}\PY{n}{swarmplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{predictors}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.25}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    As we can see from the plot, we notice that X3,X5,X6,X9,X10 and X11 are
stacked with many outliers outside the 1.5*IQR

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Q1} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{quantile}\PY{p}{(}\PY{l+m+mf}{0.25}\PY{p}{)}
\PY{n}{Q3} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{quantile}\PY{p}{(}\PY{l+m+mf}{0.75}\PY{p}{)}
\PY{n}{IQR} \PY{o}{=} \PY{n}{Q3} \PY{o}{\PYZhy{}} \PY{n}{Q1}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{IQR}\PY{p}{)}
\PY{c+c1}{\PYZsh{}fact: quantile works on numeric dtypes so df.quantile=predictors.quantile}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
X1     7.728045
X2     4.955514
X3     4.040984
X4     5.428291
X5     3.954376
X6     3.140798
X7     3.131490
X8     3.106763
X9     2.529075
X10    3.577485
X11    3.475663
X12    3.749751
dtype: float64
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{outliers\PYZus{}det}\PY{p}{(}\PY{n}{df\PYZus{}}\PY{p}{,}\PY{n}{th}\PY{p}{)}\PY{p}{:}
    
    \PY{n}{df\PYZus{}2cln} \PY{o}{=} \PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{n}{predictors\PYZus{}}\PY{o}{=}\PY{n}{df\PYZus{}2cln}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} 
    \PY{n}{Q1} \PY{o}{=} \PY{n}{df\PYZus{}2cln}\PY{o}{.}\PY{n}{quantile}\PY{p}{(}\PY{l+m+mf}{0.25}\PY{p}{)}
    \PY{n}{Q3} \PY{o}{=} \PY{n}{df\PYZus{}2cln}\PY{o}{.}\PY{n}{quantile}\PY{p}{(}\PY{l+m+mf}{0.75}\PY{p}{)}
    \PY{n}{IQR} \PY{o}{=} \PY{n}{Q3} \PY{o}{\PYZhy{}} \PY{n}{Q1}
    
    \PY{c+c1}{\PYZsh{}filter out the outliers by keeping only valid values}
    \PY{c+c1}{\PYZsh{} returns True/False list of rows (329) where false=outlier  (.any changes numbers to boolean)}
    \PY{n}{inliners}\PY{o}{=}\PY{o}{\PYZti{}}\PY{p}{(}\PY{p}{(}\PY{n}{predictors\PYZus{}}\PY{o}{\PYZlt{}} \PY{p}{(}\PY{n}{Q1} \PY{o}{\PYZhy{}} \PY{n}{th} \PY{o}{*} \PY{n}{IQR}\PY{p}{)}\PY{p}{)} \PY{o}{|}\PY{p}{(}\PY{n}{predictors\PYZus{}}\PY{o}{\PYZgt{}} \PY{p}{(}\PY{n}{Q3} \PY{o}{+} \PY{n}{th} \PY{o}{*} \PY{n}{IQR}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

    \PY{n}{outliers} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df\PYZus{}2cln}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{p}{(}\PY{n}{inliners}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
            \PY{n}{outliers}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}  \PY{c+c1}{\PYZsh{} index of the outlier}

    \PY{n}{outliers\PYZus{}idx}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{outliers}\PY{p}{)}

    \PY{n}{noise\PYZus{}df} \PY{o}{=} \PY{n}{df\PYZus{}2cln}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{n}{outliers\PYZus{}idx}\PY{p}{)}
    \PY{n}{cln\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{inliners}\PY{p}{]} \PY{c+c1}{\PYZsh{}data split\PYZsh{}1}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{noise\PYZhy{}free data split created with length = }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{cln\PYZus{}df}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outliers = }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df\PYZus{}2cln}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{cln\PYZus{}df}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data filtered percentage =}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df\PYZus{}2cln}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{cln\PYZus{}df}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df\PYZus{}2cln}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{outliers\PYZus{}idx}\PY{p}{,}\PY{n}{cln\PYZus{}df}\PY{p}{,}\PY{n}{noise\PYZus{}df}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{outliers\PYZus{}indices\PYZus{}iqr}\PY{p}{,}\PY{n}{noisefree\PYZus{}iqr\PYZus{}df}\PY{p}{,}\PY{n}{outliers\PYZus{}iqr\PYZus{}df} \PY{o}{=} \PY{n}{outliers\PYZus{}det}\PY{p}{(}\PY{n}{df}\PY{p}{,}\PY{l+m+mf}{1.5}\PY{p}{)}\PY{c+c1}{\PYZsh{}1.5 cuts more data \PYZam{} given horrible results on classification (trial and error)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
noise-free data split created with length =  261
outliers =  68
data filtered percentage =0.21
    \end{Verbatim}

    The filtered data (outliers) percentage according to 1.5*IQR is 21\% of
the whole data set

That's a large amount considering that the dataset is small for a 6
class classification

    \hypertarget{mahalanabois-distance}{%
\subsection{Mahalanabois Distance}\label{mahalanabois-distance}}

The Mahalanobis distance between an observation \((x_i)\) and the
dataset mean (\(\hat{\mu}\)) given by:

\begin{equation}
d^2_{\hat{\mu},\hat{\Sigma}}(x_i) =
(x_i-\hat{\mu})^T\hat{\Sigma}^{-1}(x_i-\hat{\mu})
\end{equation}

\$\hat{\mu}, \hat{\Sigma} \$ being respectively estimates of the dataset
mean and covariance., where \(\Sigma\) is a \(d\times d\) covariance
matrix.

The sample mean and covariance matrix can be quite sensitive to
outliers,and that's the reason why Mahalanobis distance is effective on
multivariate data is because it uses covariance between variables in
order to find the distance of two points. In other words, Mahalanobis
calculates the distance between point ``P1'' and point ``P2'' by
considering standard deviation (how many standard deviations P1 far from
P2)

Then, Multivariate outliers can be simply defined as observations having
a large squared Mahalanobis distance.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{outliers\PYZus{}maha}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{:}

    \PY{n}{df1}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{df}\PY{p}{)}
    \PY{n}{cov\PYZus{}matrix} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{cov}\PY{p}{(}\PY{n}{df1}\PY{p}{,} \PY{n}{rowvar}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} 
    \PY{n}{inv\PYZus{}cov\PYZus{}matrix} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{cov\PYZus{}matrix}\PY{p}{)}
    \PY{n}{vars\PYZus{}mean} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{df1}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
        \PY{n}{vars\PYZus{}mean}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{df1}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
    \PY{n}{diff} \PY{o}{=} \PY{n}{df1} \PY{o}{\PYZhy{}} \PY{n}{vars\PYZus{}mean}

    \PY{n}{md} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{diff}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{md}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{diff}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{inv\PYZus{}cov\PYZus{}matrix}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{diff}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}

    \PY{n}{std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{md}\PY{p}{)}
    \PY{n}{k} \PY{o}{=} \PY{l+m+mf}{2.} \PY{o}{*} \PY{n}{std} \PY{c+c1}{\PYZsh{} k = 3. * std if extreme else 2. * std}
    \PY{n}{m} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{md}\PY{p}{)}
    \PY{n}{up\PYZus{}t} \PY{o}{=} \PY{n}{m} \PY{o}{+} \PY{n}{k}
    \PY{n}{low\PYZus{}t} \PY{o}{=} \PY{n}{m} \PY{o}{\PYZhy{}} \PY{n}{k}
    \PY{n}{outliers} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{md}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{p}{(}\PY{n}{md}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{up\PYZus{}t}\PY{p}{)} \PY{o+ow}{or} \PY{p}{(}\PY{n}{md}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{low\PYZus{}t}\PY{p}{)}\PY{p}{:}
            \PY{n}{outliers}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}  \PY{c+c1}{\PYZsh{} index of the outlier}

    \PY{n}{indices\PYZus{}}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{outliers}\PY{p}{)}
    \PY{n}{outliers\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{n}{indices\PYZus{}}\PY{p}{)}

    \PY{n}{cln\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{o}{\PYZti{}}\PY{n}{outliers\PYZus{}df}\PY{p}{]}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{noise free data split created with length = }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{cln\PYZus{}df}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cleared data = }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{cln\PYZus{}df}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data filtered percentage =}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{cln\PYZus{}df}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{indices\PYZus{}}\PY{p}{,}\PY{n}{cln\PYZus{}df}\PY{p}{,}\PY{n}{outliers\PYZus{}df}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{outliers\PYZus{}indices\PYZus{}maha}\PY{p}{,} \PY{n}{inliers\PYZus{}maha\PYZus{}df}\PY{p}{,}\PY{n}{outliers\PYZus{}maha\PYZus{}df}\PY{o}{=}\PY{n}{outliers\PYZus{}maha}\PY{p}{(}\PY{n}{df}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

noise free data split created with length =  318
cleared data =  11
data filtered percentage =0.03
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print(df.index.isin(outliers\PYZus{}indices)) index of outliers}
\PY{c+c1}{\PYZsh{} print(language[outliers\PYZus{}iqr\PYZus{}df].value\PYZus{}counts(sort=False)[::\PYZhy{}1]);}
\PY{c+c1}{\PYZsh{} print(language[outliers\PYZus{}maha\PYZus{}df].value\PYZus{}counts(sort=False)[::\PYZhy{}1]);}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}

\PY{n}{maha\PYZus{}counts}\PY{o}{=}\PY{n}{language}\PY{p}{[}\PY{n}{outliers\PYZus{}maha\PYZus{}df}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{sort}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{iqr\PYZus{}counts}\PY{o}{=}\PY{n}{language}\PY{p}{[}\PY{n}{outliers\PYZus{}iqr\PYZus{}df}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{sort}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
  
\PY{n}{X\PYZus{}axis} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{language}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  
\PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{X\PYZus{}axis} \PY{o}{+} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{maha\PYZus{}counts}\PY{p}{,} \PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mahanapolis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{X\PYZus{}axis} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{iqr\PYZus{}counts}\PY{p}{,} \PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{IQR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  
\PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{X\PYZus{}axis}\PY{p}{,} \PY{n}{language}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{} of Outliers}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Outliers class distribution}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_51_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    That is the \textbf{outliers class distribution}, and as we can see,
both outliers detectors affected by the class imbalance i.e.~bias
towards the majority class

    \hypertarget{outlier-removal-tradeoff}{%
\subsection{Outlier Removal Tradeoff}\label{outlier-removal-tradeoff}}

\begin{itemize}
\tightlist
\item
  we might run the risk of information loss which will cause our models
  to have a lower accuracy specially with the limited data in hand
\item
  both outliers detectors affected by the class imbalance i.e.~bias
  towards the majority class
\item
  According to UCI, the collected data is noise free
\end{itemize}

for the abovementioned reasons, only a dataset split by removing shared
outliers between IQR and mahanapolis will be examined for classification

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mahalanabois Outliers Indices: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{outliers\PYZus{}indices\PYZus{}maha}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Mahalanabois Outliers Indices: [124 136 224 226 229 231 232 286 287 295 322]

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{IQR Outliers Indices: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{outliers\PYZus{}indices\PYZus{}iqr}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
IQR Outliers Indices: [ 14  22  27 157 162 171 194 204 213 215 219 221 224 225
226 227 228 229
 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247
 248 249 250 251 252 253 263 284 285 286 287 291 292 293 294 295 298 314
 315 316 317 318 319 320 321 322 323 324 325 326 327 328]

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{commonElements}\PY{p}{(}\PY{n}{arr}\PY{p}{)}\PY{p}{:} 
    \PY{c+c1}{\PYZsh{} initialize result with first array as a set }
    \PY{n}{result} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{arr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} 
    \PY{k}{for} \PY{n}{currSet} \PY{o+ow}{in} \PY{n}{arr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{:} 
        \PY{n}{result}\PY{o}{.}\PY{n}{intersection\PYZus{}update}\PY{p}{(}\PY{n}{currSet}\PY{p}{)} 
  
    \PY{k}{return} \PY{n+nb}{list}\PY{p}{(}\PY{n}{result}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{outliers\PYZus{}indices}\PY{o}{=}\PY{p}{[}\PY{n}{outliers\PYZus{}indices\PYZus{}iqr}\PY{p}{,}\PY{n}{outliers\PYZus{}indices\PYZus{}maha}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{index}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Counter of common outliers between IQR \PYZam{} Mahalanobis:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{commonElements}\PY{p}{(}\PY{n}{outliers\PYZus{}indices}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Counter of common outliers between IQR \& Mahalanobis: 9
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{noise} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{n}{commonElements}\PY{p}{(}\PY{n}{outliers\PYZus{}indices}\PY{p}{)}\PY{p}{)}
\PY{n}{noisefree\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{o}{\PYZti{}}\PY{n}{noise}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Noise\PYZhy{}free dataset split of Shape:}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ is created}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{noisefree\PYZus{}df}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{}         print(\PYZsq{}Length of X (train): \PYZob{}\PYZcb{} | Length of y (train): \PYZob{}\PYZcb{}\PYZsq{}.format(len(X\PYZus{}train), len(y\PYZus{}train)))}

\PY{c+c1}{\PYZsh{}note that indexes of \PYZgt{}320 still appears because idx is related to rows and intermediate idx deleted==\PYZgt{} idx.shap=261}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Noise-free dataset split of Shape:(320, 13) is created
    \end{Verbatim}

    \hypertarget{multicollinearity-detection}{%
\section{Multicollinearity
Detection}\label{multicollinearity-detection}}

    Collinearity is a linear association between two variables.
Multicollinearity is a situation where the correlations among subsets of
the variables

Multicollinearity is not a big issue in classification problems since it
is not affecting the prediction power, rather we use it for feature
selection to select subset of the most relevant features that helps in
classification

Two methods of detection will be examined against the dataset

\hypertarget{pearson-correlation}{%
\subsection{Pearson correlation}\label{pearson-correlation}}

Pearson correlation coefficient (PCC) is a measure of linear correlation
between two sets of data. It is the ratio between the covariance of two
variables and the product of their standard deviations; thus it is
essentially a normalized measurement of the covariance, such that the
result always has a value between 1 and 1. As with covariance itself,
the measure can only reflect a linear correlation of variables, and
ignores many other types of relationship or correlation.

Given a pair of random variables \((X,Y)\) the formula for Pearson
correlation coefficient \(\rho\) is:

\begin{equation}
\label{eq:dotp}
\rho_{X,Y} = \frac {cov(X,Y)} {\sigma_X \sigma_Y}, 
\end{equation} where \(\sigma\) is the standard deviation \ldots{}

In general, \textbf{an absolute correlation coefficient of
\textgreater0.7 among two or more predictors indicates the presence of
multicollinearity}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}class to return the matrix of p\PYZhy{}value of predictors }
\PY{c+c1}{\PYZsh{}The P\PYZhy{}value is the probability that you would have found the current result if the correlation coefficient were in }
\PY{c+c1}{\PYZsh{}fact zero (null hypothesis). If this probability is lower than the conventional 5\PYZpc{} (P\PYZlt{}0.05) the }
\PY{c+c1}{\PYZsh{}correlation coefficient is called statistically significant}
\PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k+kn}{import} \PY{n}{stats}
\PY{k}{def} \PY{n+nf}{corr\PYZus{}sig}\PY{p}{(}\PY{n}{predictors}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{n}{p\PYZus{}matrix} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{predictors}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{predictors}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{predictors}\PY{o}{.}\PY{n}{columns}\PY{p}{:}
        \PY{k}{for} \PY{n}{col2} \PY{o+ow}{in} \PY{n}{predictors}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{col}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{columns}\PY{p}{:}
            \PY{n}{\PYZus{}} \PY{p}{,} \PY{n}{p} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{pearsonr}\PY{p}{(}\PY{n}{predictors}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{,}\PY{n}{predictors}\PY{p}{[}\PY{n}{col2}\PY{p}{]}\PY{p}{)}
            \PY{n}{p\PYZus{}matrix}\PY{p}{[}\PY{n}{predictors}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{to\PYZus{}list}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{index}\PY{p}{(}\PY{n}{col}\PY{p}{)}\PY{p}{,}\PY{n}{predictors}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{to\PYZus{}list}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{index}\PY{p}{(}\PY{n}{col2}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{p}
    \PY{k}{return} \PY{n}{p\PYZus{}matrix}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Corr.Plot of Example Data with only Sig. Correlations (plotting with only significant p\PYZhy{}value correlation (alpha \PYZlt{} .05))}

\PY{n}{p\PYZus{}values} \PY{o}{=} \PY{n}{corr\PYZus{}sig}\PY{p}{(}\PY{n}{predictors}\PY{p}{)}

\PY{n}{mask} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{invert}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{tril}\PY{p}{(}\PY{n}{p\PYZus{}values}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.05}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}0.000000000000000000000000001}

\PY{c+c1}{\PYZsh{}Compute pairwise correlation of columns .corr() default method=pearson}
\PY{c+c1}{\PYZsh{}obviously another way is to return the matrix of \PYZus{} in the above class}
\PY{n}{corrmat} \PY{o}{=} \PY{n}{predictors}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)} 
\PY{n}{top\PYZus{}corr\PYZus{}predictors}\PY{o}{=} \PY{n}{corrmat}\PY{o}{.}\PY{n}{index}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

\PY{n}{g}\PY{o}{=}\PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{top\PYZus{}corr\PYZus{}predictors}\PY{p}{]}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}\PY{n}{cmap}\PY{o}{=}\PY{n}{sns}\PY{o}{.}\PY{n}{diverging\PYZus{}palette}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{75}\PY{p}{,} \PY{n}{l}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,}\PY{n}{n}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}\PY{p}{,} \PY{n}{mask}\PY{o}{=}\PY{n}{mask}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_62_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    considering an absolute correlation coefficient of \textgreater0.7 from
the heatmap following strong correlations found:

\begin{itemize}
\tightlist
\item
  X5,X6 with X9
\item
  X4,X5,X6 with X10
\item
  X9 with X10
\end{itemize}

Apparently, will lose info if X9 or X10 is droppped since they go
together, so the right decision is to drop X4,X5,X6

    \hypertarget{hieracichal-clustering}{%
\subsection{Hieracichal clustering}\label{hieracichal-clustering}}

It's a type of clustering requires the user to specify a measure of
dissimilarity between (disjoint) groups of observations, based on the
pairwise dissimilarities among the observations in the two groups. As
the name suggests, they produce hierarchical representations in which
the clusters at each level of the hierarchy are created by merging
clusters at the next lower level. At the lowest level, each cluster
contains a single observation. At the highest level there is only one
cluster containing all of the data.

Strategies for hierarchical clustering divide into two basic paradigms:
agglomerative (bottom-up) and divisive (top-down).

\textbf{Agglomerative} strategies start at the bottom and at each level
recursively merge a selected pair of clusters into a single cluster.
This produces a grouping at the next higher level with one less cluster.
The pair chosen for merging consist of the two groups with the smallest
intergroup dissimilarity

Most agglomerative and some divisive methods (when viewed bottomup)
possess a monotonicity property. That is, the dissimilarity between
merged clusters is monotone increasing with the level of the merger.
Thus the binary tree can be plotted so that the height of each node is
proportional to the value of the intergroup dissimilarity between its
two daughters. The terminal nodes representing individual observations
are all plotted at zero height. This type of graphical display is called
a \textbf{dendrogram}.

\textbf{Figure shows agglomerative clustering and dendrogram with
average linkage}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{94}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{g} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{clustermap}\PY{p}{(}\PY{n}{predictors}\PY{p}{,} \PY{n}{row\PYZus{}cluster}\PY{o}{=} \PY{k+kc}{False}\PY{p}{,}  \PY{n}{metric}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{correlation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    `X4',`X5',`X6' can be removed according to Pearson correlation

`X4',`X5',`X6',`X7' can be removed according to dendogram

we use trial and error to choose the right combination gives the best
classification results

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{95}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}trial and error}
\PY{c+c1}{\PYZsh{} @knn (k=3)}
\PY{c+c1}{\PYZsh{} df\PYZus{}colfree=df.drop(columns=[\PYZsq{}X4\PYZsq{},\PYZsq{}X5\PYZsq{},\PYZsq{}X6\PYZsq{},\PYZsq{}X7\PYZsq{}],axis=1) \PYZsh{}bad}

\PY{c+c1}{\PYZsh{}all three comp }
\PY{c+c1}{\PYZsh{} df\PYZus{}colfree=df.drop(columns=[\PYZsq{}X4\PYZsq{},\PYZsq{}X5\PYZsq{},\PYZsq{}X6\PYZsq{}],axis=1)\PYZsh{}0.6752641623609366}
\PY{c+c1}{\PYZsh{} df\PYZus{}colfree=df.drop(columns=[\PYZsq{}X4\PYZsq{},\PYZsq{}X5\PYZsq{},\PYZsq{}X7\PYZsq{}],axis=1)\PYZsh{}F1 score on test set: 0.7674608993157381}
\PY{c+c1}{\PYZsh{} df\PYZus{}colfree=df.drop(columns=[\PYZsq{}X4\PYZsq{},\PYZsq{}X6\PYZsq{},\PYZsq{}X7\PYZsq{}],axis=1)\PYZsh{}F1 score on test set: 0.7898923444976076}
\PY{c+c1}{\PYZsh{} df\PYZus{}colfree=df.drop(columns=[\PYZsq{}X5\PYZsq{},\PYZsq{}X6\PYZsq{},\PYZsq{}X7\PYZsq{}],axis=1)\PYZsh{}0.7070455550781195}

\PY{c+c1}{\PYZsh{}two only}
\PY{n}{df\PYZus{}colfree}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X6}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X7}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}0.8500541125541126}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{dimensionality-reduction}{%
\section{Dimensionality Reduction}\label{dimensionality-reduction}}

    Principal component analysis (PCA) is is unsupervised learning that uses
an orthogonal transformation to convert the variables of a dataset into
a new set of variables which are linearly uncorrelated. The principal
components are ranked according to the variance of data along them.

\begin{definition}\
  Given observations $x_i^{(k)}$ of random variables $X^{(k)}$, want
  to find <span class="mark">linearly uncorrelated</span> principal components.

  - Write $X = (T{x}_1 | \cdots | T{x}_\ell) \in RR^{N \times \ell}$.
  
  - Calculate Singular Value Decomposition $X = U S V^t$, 
  
  - Then the principal components are the variables \[ Y^{(j)} = \sum_k U_{kj} X^{(k)}. \]

\end{definition}

Most of the variance is captured by \(Y^{(1)}\); second to most is
captured by \(Y^{(2)}\); and so on

\textbf{For the above mentioned procedure to be correct, data need to be
centered and scaled}

This technique can be used to reduce the dimensionality of the dataset
by considering just the most important principal components and wether
it will help classifing the accent recognition dataset i.e.~if the
variance help in seperating classes

sklearn.decomposition.PCA class uses Singular Value Decomposition of the
data to project it to a lower dimensional space. The input data is
centered but not scaled for each feature before applying the SVD.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k+kn}{import} \PY{n}{PCA}

\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{5}
\PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{}the following statemend can be done through}
\PY{c+c1}{\PYZsh{}from sklearn.preprocessing import scale, normalize}
\PY{c+c1}{\PYZsh{}norm\PYZus{}predictors = normalize(predictors)}

\PY{n}{norm\PYZus{}predictors} \PY{o}{=} \PY{p}{(}\PY{n}{predictors} \PY{o}{\PYZhy{}} \PY{n}{predictors}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{predictors}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}pca.fit\PYZus{}transform does not normalize data automatically}

\PY{n}{X\PYZus{}pca} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{norm\PYZus{}predictors}\PY{p}{)}
\PY{n}{X\PYZus{}pca} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}pca}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PC}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{x} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{)}\PY{p}{]}\PY{p}{)} 

\PY{n}{X\PYZus{}pca}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language\PYZus{}coded}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{105}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sum of explained variance ratio using }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ PCA components: }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{)}\PY{p}{,}\PY{n+nb}{round}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{explained variance ratio by each PCA component}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Sum of explained variance ratio using 5 PCA components:  0.86

explained variance ratio by each PCA component
[0.48029376 0.16148163 0.09600608 0.0736275  0.04723642]
    \end{Verbatim}

    \begin{itemize}
\tightlist
\item
  4 Principal components are enough to explain about 79\%
\item
  5 Principal components are enough to explain about 86\%
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Individual component}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{cumsum}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cumulative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}1,2,3,4 to avoid PCA1 is at 0 in xscale}

\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Proportion of Variance Explained}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Principal Component}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{l+m+mf}{0.8}\PY{p}{,}\PY{l+m+mf}{5.2}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.05}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_73_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Let us see the effect of original predictors on each principal
components

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Effect of original predictors (variables) on each components}
\PY{c+c1}{\PYZsh{}too dark = too positive effect , too light= too negative effect}
\PY{c+c1}{\PYZsh{}to do:just abs() coz all we need now is the effect whether its + or \PYZhy{}}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{p}{,} 
                 \PY{n}{linewidths}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,}
                 \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                 \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Blues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{yticklabels}\PY{o}{=}\PY{p}{[} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PCA}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{v}\PY{p}{)} \PY{k}{for} \PY{n}{v} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{pca}\PY{o}{.}\PY{n}{n\PYZus{}components\PYZus{}}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{,}
                 \PY{n}{xticklabels}\PY{o}{=}\PY{n+nb}{list}\PY{p}{(}\PY{n}{predictors}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{,}
                 \PY{n}{cbar\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{orientation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{horizontal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{setp}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}yticklabels}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}aspect}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{equal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_75_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    PCA1 is increased with X5 and X10 and decreased with X4,X6,X9. This
suggests that these five coeffiecient goes together, If one increases,
then the remaining ones tend to increase/decrease. So this component
PCA1 can be viewed as a measure of the quality of X4,X5,X6,X9 and X10

PCA1 is more affected by X9,X10 than X4,X5,X6 (the same result we
reached in the multicollinearity section)

    \hypertarget{classification}{%
\section{Classification}\label{classification}}

    \hypertarget{imbalanced-dataset-classification}{%
\subsection{Imbalanced Dataset
Classification}\label{imbalanced-dataset-classification}}

    The machine-learning literature on this topic has essentially developed
two common solution strategies: 1. Restore balance on the training set
by \textbf{undersampling} the large class or by \textbf{oversampling}
the small class, to prevent bias from arising in the first place. 2.
modify the \textbf{costs of misclassification} to prevent bias by
penalizing learning algorithms that increase the cost of classification
mistakes on the minority class. this technique is implemented in python
using the argument \textbf{class\_weight='balanced'} within classifier
models during training to penalize mistakes on the minority class by an
amount proportional to how under-represented it is.We also want to
include the argument \textbf{probability=True} if we want to enable
probability estimates for SVM algorithms.

    \hypertarget{classification-metric}{%
\subsection{Classification Metric}\label{classification-metric}}

    In learning imbalanced data, the overall classification accuracy is
often not an appropriate measure of performance. A trivial classifier
that predicts every case as the majority class can still achieve very
high accuracy.

Metrics such as true negative rate, true positive rate, precision,
recall, and F-measure are commonly used

Since there is no distinction between whether Precision is more
important or Recall in the accent recognition case, we combine them

\textbf{F1-measure} is the harmonic mean of precision and recall:

In practice, when we try to increase the precision of our model, the
recall goes down and vice-versa. The F1-score captures both the trends
in a single value.

\begin{equation}
F_1score = 2 \left[ \frac {Precision \times Recall} {Precision+ Recall} \right]
\end{equation}

In the multi-class case, there are three ways to generalize F1 scores: -
\textbf{macro-averaged} F1, and it weighs each class equally. -
\textbf{micro-averaged} F1, and it weighs each sample equally. -
\textbf{Weighted-averaged} each classes's contribution to the average is
weighted by its size

To give equal weight to each class, use macro-averaging

    \hypertarget{k-fold-cross-validation}{%
\subsection{K-fold Cross-Validation}\label{k-fold-cross-validation}}

    for estimating prediction error \textbf{K-fold cross-validation} uses
part of the available data to fit the model, and a different part to
test it. We split the data into K roughly equal-sized parts and for the
kth part (third above), we fit the model to the other K 1 parts of the
data, and calculate the prediction error of the fitted model when
predicting the kth part of the data. We do this for k = 1, 2,\ldots,K
and combine the K estimates of prediction error.

Let \(\) : \{1,\ldots,N\}  \{1,\ldots,K\} be an indexing function that
indicates the partition to which observation \(i\) is allocated by the
randomization. Denote by \(f^{k}(x)\) the fitted function, computed
with the \(kth\) part of the data removed. Then the cross-validation
estimate of prediction error is \begin{equation}\label{eq:}
CV(f)= \sum_{i=1}^{N} L(y_i,f^{-k(i)}(x_i))
\end{equation}

Typical choices of K are 5 or 10

sklearn package provides the class \textbf{StratifiedKFold}; a
cross-validation object returns stratified folds. The folds are made by
preserving the percentage of samples for each class as shown below

    

    \hypertarget{classification-models}{%
\subsection{Classification Models}\label{classification-models}}

    The following classification models gives competent results on the
speaker accent recognition dataset: \#\#\# K-Nearest Neighbour:
Nearest-neighbor methods use those observations in the training set
\(T\) closest in input space to \(x\) to form \(\hat{Y}\) .
Specifically, the k-nearest neighbor fit for \(\hat{Y}\) is defined as
follows:

{[}\hat{Y}(x)=\frac{1}{k} \sum\_\{\{x\_i\}\in {N_k (x)}\} y\_i{]}

where \(N_k(x)\) is the neighborhood of \(x\) defined by the \(k\)
closest points \(x_i\) in the training sample. Closeness implies a
metric, Euclidean distance is the default choice in sklearn nearest
neighbour algorithm. So, in words, we find the \(k\) observations with
\(x_i\) closest to \(x\) in input space, and average their responses.

thae k-nearest-neighbor fits have a single parameter, the number of
neighbors k, as k increases bias increases and variance decreases and so
a small value of k could lead to \textbf{overfitting} as well as a big
value of k can lead to \textbf{underfitting}

    \hypertarget{support-vector-machine}{%
\subsubsection{Support Vector Machine:}\label{support-vector-machine}}

When the classes are not separable by a linear boundary, the support
vector machine (SVM) classifier maps sample \(x_i\) into a feature space
of higher dimensions \(\phi (x)\) in which the classes can be linearly
separated. This results in a non-linear decision boundary in the
original dimensions.

As the vectors \(x_i\) appear as inner products in the optimization
problem to find the saperating hyperplane, the mapping function
\(\phi(x)\) does not need to be explicitly specified, rather requires
only knowledge of the kernel function:

{[}K(x\_1,x\_2)=\phi(x\_1)\^{}T(x\_2){]}

The effective hyperparameters of a SVM to be tuned and implemented using
sklearn library are: In the hyperparameter tuning we consider the
following two kernels:

1- the kerenel function and the functions chosen are the following: -
linear kernel: \(K(x_1,x_2)= x_1x_2\) - Radial Basis Function (RBF):
\(K(x_1,x_2)=exp (\gammax_1x_2^2)\)

2- C: the cost parameter, a large value of C will discourage any
positive slack variables and lead to an overfit wiggly boundary in the
original feature space; a small value of C will causes \(f(x)\) and
hence the boundary to be smoother.

3- Gamma (RBF parameter): as parameter C, The higher the gamma the more
chance of overfitting

    \hypertarget{random-forest}{%
\subsubsection{Random Forest}\label{random-forest}}

An average of \(B\) i.i.d. random variables, each with variance
\(\sigma^2\), has variance \(\frac{1}{B} \sigma^2\). If the variables
are simply i.d. (identically distributed, but not necessarily
independent) with positive pairwise correlation \(\rho\), the variance
of the average to be optimized is

{[}\rho \sigma\^{}2+\frac{1-\rho}{B} \sigma\^{}2{]}

The idea in \textbf{random forests} is to improve the variance reduction
of bagging by reducing the correlation between the trees, without
increasing the variance too much. This is achieved in the tree-growing
process (to the bootstrapped data) through random selection of the input
variables \((m =<p)\)(with selecting best variables at each step as
candidate for splitting). When used for classification, a random forest
obtains a class vote from each tree, and then classifies using majority
vote

For classification, the default value for m is \(\sqrt{p}\) and the
minimum node size is one.

Random forest parameters to be considered in implementation using
sklearn library: - Number of trees in random forest - Number of features
to consider at every split - Maximum number of levels in tree - Minimum
number of samples required to split a node - Minimum number of samples
required at each leaf node - Method of selecting samples for training
each tree (bootstrap or not)

\textbf{Note: Linear Discriminant Analysis and Logistic Regression are
used along the abovementioned classifier, but since both gave poor
results, they are excluded from the report. Same goes for evaluation
metrics precision and recall}

    \hypertarget{implementation-framework}{%
\subsubsection{Implementation
Framework}\label{implementation-framework}}

    

    The figure above shows the classification framework for the imbalanced
and balanced cases. Stratified \textbf{10-folds} is performed for
cross-validation

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split} \PY{k}{as} \PY{n}{tts}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k+kn}{import} \PY{n}{KNeighborsClassifier}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k+kn}{import} \PY{n}{RandomForestClassifier}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{discriminant\PYZus{}analysis} \PY{k+kn}{import} \PY{n}{LinearDiscriminantAnalysis}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{LogisticRegression}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{GridSearchCV}\PY{p}{,} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{,}\PY{n}{KFold}\PY{p}{,} \PY{n}{StratifiedKFold}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{precision\PYZus{}score}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{,} \PY{n}{f1\PYZus{}score}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}
\PY{k+kn}{import} \PY{n+nn}{time} \PY{k}{as} \PY{n+nn}{time}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.3}
\PY{n}{SEED}\PY{o}{=}\PY{l+m+mi}{20} \PY{c+c1}{\PYZsh{}to correcty compare results of the different datasets splits use the same random\PYZus{}state seed (to pick same instances for df,pca,colfree) only noisefree can\PYZsq{}t pick the same indices}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{test\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{,} \PY{n}{test\PYZus{}labels} \PY{o}{=} \PY{n}{tts}\PY{p}{(}\PY{n}{predictors}\PY{p}{,}\PY{n}{target}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{n}{test\PYZus{}size}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{target}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
\PY{n}{train\PYZus{}colfree}\PY{p}{,} \PY{n}{test\PYZus{}colfree}\PY{p}{,} \PY{n}{colfree\PYZus{}train\PYZus{}labels}\PY{p}{,} \PY{n}{colfree\PYZus{}test\PYZus{}labels} \PY{o}{=} \PY{n}{tts}\PY{p}{(}\PY{n}{df\PYZus{}colfree}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language\PYZus{}coded}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{df\PYZus{}colfree}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language\PYZus{}coded}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{n}{test\PYZus{}size}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{df\PYZus{}colfree}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language\PYZus{}coded}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
\PY{n}{train\PYZus{}noisefree}\PY{p}{,} \PY{n}{test\PYZus{}noisefree}\PY{p}{,} \PY{n}{trainlabels\PYZus{}noisefree}\PY{p}{,} \PY{n}{testlabels\PYZus{}noisefree} \PY{o}{=} \PY{n}{tts}\PY{p}{(}\PY{n}{noisefree\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{12}\PY{p}{]}\PY{p}{,} \PY{n}{noisefree\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{n}{test\PYZus{}size}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{noisefree\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}prepare pca\PYZhy{}transformed dataset for classification}
\PY{n}{norm\PYZus{}predictors} \PY{o}{=} \PY{p}{(}\PY{n}{train\PYZus{}data} \PY{o}{\PYZhy{}} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\PY{n}{pca\PYZus{}train} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{norm\PYZus{}predictors}\PY{p}{)}
\PY{n}{norm\PYZus{}predictors} \PY{o}{=} \PY{p}{(}\PY{n}{test\PYZus{}data} \PY{o}{\PYZhy{}} \PY{n}{test\PYZus{}data}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{test\PYZus{}data}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\PY{n}{pca\PYZus{}test} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{norm\PYZus{}predictors}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{datasets}\PY{o}{=}\PY{p}{\PYZob{}}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original dataset}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{n}{train\PYZus{}data}\PY{p}{,}\PY{n}{train\PYZus{}labels}\PY{p}{,}\PY{n}{test\PYZus{}data}\PY{p}{,}\PY{n}{test\PYZus{}labels}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PCA}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{n}{pca\PYZus{}train}\PY{p}{,}\PY{n}{train\PYZus{}labels}\PY{p}{,} \PY{n}{pca\PYZus{}test}\PY{p}{,}\PY{n}{test\PYZus{}labels}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{noise\PYZhy{}free}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{n}{train\PYZus{}noisefree}\PY{p}{,} \PY{n}{trainlabels\PYZus{}noisefree}\PY{p}{,} \PY{n}{test\PYZus{}noisefree}\PY{p}{,} \PY{n}{testlabels\PYZus{}noisefree}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coll\PYZhy{}free}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{n}{train\PYZus{}colfree}\PY{p}{,} \PY{n}{colfree\PYZus{}train\PYZus{}labels}\PY{p}{,} \PY{n}{test\PYZus{}colfree}\PY{p}{,}\PY{n}{colfree\PYZus{}test\PYZus{}labels}\PY{p}{]}
\PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{imbalanced-dataset-classification}{%
\subsubsection{Imbalanced dataset
classification}\label{imbalanced-dataset-classification}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Number of trees in random forest}
\PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{start} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{stop} \PY{o}{=} \PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{num} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{]} 
\PY{c+c1}{\PYZsh{} Number of features to consider at every split}
\PY{n}{max\PYZus{}features} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sqrt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{c+c1}{\PYZsh{} Maximum number of levels in tree}
\PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{110}\PY{p}{,} \PY{n}{num} \PY{o}{=} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{]} \PY{c+c1}{\PYZsh{}list(range(10, 111, 10)) + [None]}
\PY{n}{max\PYZus{}depth}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{k+kc}{None}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Minimum number of samples required to split a node}
\PY{n}{min\PYZus{}samples\PYZus{}split} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}
\PY{c+c1}{\PYZsh{} Minimum number of samples required at each leaf node}
\PY{n}{min\PYZus{}samples\PYZus{}leaf} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}
\PY{c+c1}{\PYZsh{} Method of selecting samples for training each tree}
\PY{n}{bootstrap} \PY{o}{=} \PY{p}{[}\PY{k+kc}{True}\PY{p}{,} \PY{k+kc}{False}\PY{p}{]}
\PY{n}{criterion} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gini}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{c+c1}{\PYZsh{} random\PYZus{}state int, RandomState instance, default=None}

\PY{n}{rf\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{n\PYZus{}estimators}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{criterion}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{criterion}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}features}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}depth}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{min\PYZus{}samples\PYZus{}split}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{min\PYZus{}samples\PYZus{}leaf}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bootstrap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{bootstrap}\PY{p}{\PYZcb{}}

\PY{n}{knears\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n}{log\PYZus{}reg\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{penalty}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{]}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{solver}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sag}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saga}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{liblinear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{newton\PYZhy{}cg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lbfgs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} 
                 \PY{p}{\PYZcb{}}
\PY{n}{svc\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} 
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
             \PY{p}{\PYZcb{}}

\PY{n}{lda\PYZus{}params}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{solver}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lsqr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eigen}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{params} \PY{o}{=} \PY{p}{\PYZob{}}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{knears\PYZus{}params}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{log\PYZus{}reg\PYZus{}params}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{svc\PYZus{}params}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{rf\PYZus{}params}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{lda\PYZus{}params}\PY{p}{,}
         \PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}basline classifiers}
\PY{n}{classifiers} \PY{o}{=} \PY{p}{\PYZob{}}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{KNearest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,}       
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LogisiticRegression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Support Vector Classifier}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{SVC}\PY{p}{(}\PY{p}{)}\PY{p}{,}         \PY{c+c1}{\PYZsh{}  (probability=True) is compatible with imbalanced dataset }
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Rforest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{RandomForestClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,}        \PY{c+c1}{\PYZsh{}random\PYZus{}state= 0}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LDA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{LinearDiscriminantAnalysis}\PY{p}{(}\PY{p}{)}\PY{p}{,}
              \PY{p}{\PYZcb{}}    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} cv parameters}
\PY{n}{splits}\PY{o}{=}\PY{l+m+mi}{10}
\PY{n}{cv\PYZus{}} \PY{o}{=} \PY{n}{StratifiedKFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{n}{splits}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{73}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{imbalanced\PYZus{}clf} \PY{p}{(}\PY{n}{cv}\PY{o}{=}\PY{n}{cv\PYZus{}}\PY{p}{,}\PY{n}{ds}\PY{o}{=}\PY{n}{datasets}\PY{p}{,}\PY{n}{clf}\PY{o}{=}\PY{n}{classifiers}\PY{p}{,}\PY{n}{par}\PY{o}{=}\PY{n}{params}\PY{p}{)}\PY{p}{:}
    \PY{n}{df\PYZus{}results} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{ds}\PY{p}{:}
        \PY{n}{X\PYZus{}train}\PY{o}{=}\PY{n}{ds}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{y\PYZus{}train}\PY{o}{=}\PY{n}{ds}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{X\PYZus{}test}\PY{o}{=}\PY{n}{ds}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
        \PY{n}{y\PYZus{}test}\PY{o}{=}\PY{n}{ds}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}

        \PY{n}{accuracy\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{precision\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{recall\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{f1\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{auc\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        
        \PY{k}{for} \PY{p}{(}\PY{n}{key1}\PY{p}{,}\PY{n}{classifier}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{key2}\PY{p}{,}\PY{n}{parameter}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{par}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{n}{start\PYZus{}time} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
            \PY{n}{grid}\PY{o}{=}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{classifier}\PY{p}{,} \PY{n}{parameter}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{cv}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f1\PYZus{}weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{grid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
            \PY{n}{best\PYZus{}est} \PY{o}{=} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
            \PY{n}{prediction} \PY{o}{=} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
            
            \PY{n}{training\PYZus{}score} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}est}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f1\PYZus{}weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{cv}\PY{p}{)}
            \PY{n}{row} \PY{o}{=} \PY{p}{\PYZob{}}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dataset split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{key}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{classifier}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}class\PYZus{}\PYZus{}}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{run\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start\PYZus{}time}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{60}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 CV score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{training\PYZus{}score}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precision score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best parameters}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{,}
                   \PY{p}{\PYZcb{}}
            \PY{n}{df\PYZus{}results} \PY{o}{=} \PY{n}{df\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{row}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
    \PY{k}{return} \PY{n}{df\PYZus{}results}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{661}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{imbalanced\PYZus{}clf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{661}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
       Dataset split  F1 CV score  F1 score on test set  \textbackslash{}
8                PCA         0.55                0.4253
7                PCA         0.47                0.4257
9                PCA         0.44                0.4450
6                PCA         0.50                0.4717
5                PCA         0.57                0.5244
14        noise-free         0.67                0.6723
19         coll-free         0.60                0.7086
11        noise-free         0.69                0.7363
4   Original dataset         0.67                0.7446
1   Original dataset         0.69                0.7489
16         coll-free         0.65                0.7598
18         coll-free         0.74                0.7815
15         coll-free         0.75                0.7904
2   Original dataset         0.84                0.8015
3   Original dataset         0.79                0.8030
0   Original dataset         0.80                0.8102
12        noise-free         0.82                0.8282
10        noise-free         0.77                0.8337
17         coll-free         0.79                0.8380
13        noise-free         0.72                0.8581

    Precision score on test set  Recall score on test set  \textbackslash{}
8                        0.3931                    0.4747
7                        0.4127                    0.5253
9                        0.4705                    0.5051
6                        0.5478                    0.5051
5                        0.5242                    0.5253
14                       0.6707                    0.6875
19                       0.7206                    0.7172
11                       0.7420                    0.7396
4                        0.7787                    0.7475
1                        0.7589                    0.7475
16                       0.7687                    0.7677
18                       0.7933                    0.7879
15                       0.8002                    0.7879
2                        0.8073                    0.8081
3                        0.8237                    0.8081
0                        0.8195                    0.8081
12                       0.8294                    0.8333
10                       0.8449                    0.8333
17                       0.8535                    0.8384
13                       0.8826                    0.8646

                                                              best parameters  \textbackslash{}
8   \{'bootstrap': False, 'criterion': 'entropy', 'max\_depth': 40,
'max\_features': 'auto', 'min\_samples\_leaf': 1, 'min\_samples\_split': 2,
'n\_estimators': 500\}
7
\{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'\}
9
\{'solver': 'lsqr'\}
6
\{'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'\}
5
\{'n\_neighbors': 5\}
14
\{'solver': 'lsqr'\}
19
\{'solver': 'lsqr'\}
11
\{'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'\}
4
\{'solver': 'lsqr'\}
1
\{'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'\}
16
\{'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'\}
18  \{'bootstrap': False, 'criterion': 'entropy', 'max\_depth': 40,
'max\_features': 'auto', 'min\_samples\_leaf': 1, 'min\_samples\_split': 2,
'n\_estimators': 500\}
15
\{'n\_neighbors': 3\}
2
\{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'\}
3   \{'bootstrap': False, 'criterion': 'entropy', 'max\_depth': 40,
'max\_features': 'auto', 'min\_samples\_leaf': 1, 'min\_samples\_split': 2,
'n\_estimators': 500\}
0
\{'n\_neighbors': 3\}
12
\{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'\}
10
\{'n\_neighbors': 3\}
17
\{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'\}
13  \{'bootstrap': False, 'criterion': 'entropy', 'max\_depth': 40,
'max\_features': 'auto', 'min\_samples\_leaf': 1, 'min\_samples\_split': 2,
'n\_estimators': 500\}

                         model run\_time
8       RandomForestClassifier     0.23
7                          SVC      0.0
9   LinearDiscriminantAnalysis      0.0
6           LogisticRegression     0.03
5         KNeighborsClassifier      0.0
14  LinearDiscriminantAnalysis      0.0
19  LinearDiscriminantAnalysis      0.0
11          LogisticRegression     0.29
4   LinearDiscriminantAnalysis      0.0
1           LogisticRegression     0.21
16          LogisticRegression     0.12
18      RandomForestClassifier     0.26
15        KNeighborsClassifier      0.0
2                          SVC      0.0
3       RandomForestClassifier     0.25
0         KNeighborsClassifier      0.0
12                         SVC      0.0
10        KNeighborsClassifier      0.0
17                         SVC      0.0
13      RandomForestClassifier     0.26
\end{Verbatim}
\end{tcolorbox}
        
    Results for the imbalanced case shows the following:

\begin{itemize}
\tightlist
\item
  Both original dataset and outliers free dataset gives already high
  performance results on SVM, KNN and Random Forest
\item
  Outliers-free dataset outperformed the original dataset in all three
  classifiers
\item
  Random forest and KNN shows sign of overfitting in the Outliers-free
  case
\item
  Performance dropped significantly for the same classifiers on
  Collinearity-free dataset except for RBF (overfitting)
\item
  PCA-reduced datasets gave very poor results

  \begin{itemize}
  \tightlist
  \item
    PCA for MFCC not meaningful since MFCC is already a transformed data
  \item
    In classification problem, when the differentiating characteristics
    of the classes are not reflected in variance of the variables, PCA
    may not be a good choice of data processing. This is because PCA
    does not take into account class information when calculating the
    principal components.
  \end{itemize}
\item
  LDA and Logistic regression are not optimal classifiers for the
  dataset (a sign of non-linear decision boundary between classes).
  Moreover:

  \begin{itemize}
  \tightlist
  \item
    as discussed before, predictors are coming from non gaussian
    distribution (another possible reason for the LDA to fail)
  \item
    LR does not perform well in the presence of collinearity
  \end{itemize}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{60}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}learning curve plot (source sklearn)}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{learning\PYZus{}curve}

\PY{k}{def} \PY{n+nf}{plot\PYZus{}learning\PYZus{}curve}\PY{p}{(}\PY{n}{estimator1}\PY{p}{,} \PY{n}{title1}\PY{p}{,} \PY{n}{estimator2}\PY{p}{,} \PY{n}{title2}\PY{p}{,} \PY{n}{estimator3}\PY{p}{,} \PY{n}{title3}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{ylim}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
                        \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{train\PYZus{}sizes}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{.}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}\PY{p}{:}

    \PY{n}{f}\PY{p}{,} \PY{p}{(}\PY{n}{ax1}\PY{p}{,} \PY{n}{ax2}\PY{p}{,} \PY{n}{ax3}\PY{p}{)} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{k}{if} \PY{n}{ylim} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{o}{*}\PY{n}{ylim}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} First Estimator}
    \PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{test\PYZus{}scores} \PY{o}{=} \PY{n}{learning\PYZus{}curve}\PY{p}{(}
        \PY{n}{estimator1}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{cv}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f1\PYZus{}weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{,} \PY{n}{train\PYZus{}sizes}\PY{o}{=}\PY{n}{train\PYZus{}sizes}\PY{p}{)}
    \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{train\PYZus{}scores\PYZus{}std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{test\PYZus{}scores\PYZus{}std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{\PYZhy{}} \PY{n}{train\PYZus{}scores\PYZus{}std}\PY{p}{,}
                     \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{+} \PY{n}{train\PYZus{}scores\PYZus{}std}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                     \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{\PYZhy{}} \PY{n}{test\PYZus{}scores\PYZus{}std}\PY{p}{,}
                     \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{+} \PY{n}{test\PYZus{}scores\PYZus{}std}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}2492ff}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores\PYZus{}mean}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}ff9124}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{test\PYZus{}scores\PYZus{}mean}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}2492ff}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cross\PYZhy{}validation score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{title1}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training size (m)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{best}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Second Estimator }
    \PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{test\PYZus{}scores} \PY{o}{=} \PY{n}{learning\PYZus{}curve}\PY{p}{(}
        \PY{n}{estimator2}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{cv}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f1\PYZus{}weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{,} \PY{n}{train\PYZus{}sizes}\PY{o}{=}\PY{n}{train\PYZus{}sizes}\PY{p}{)}
    \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{train\PYZus{}scores\PYZus{}std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{test\PYZus{}scores\PYZus{}std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{\PYZhy{}} \PY{n}{train\PYZus{}scores\PYZus{}std}\PY{p}{,}
                     \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{+} \PY{n}{train\PYZus{}scores\PYZus{}std}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                     \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}ff9124}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{\PYZhy{}} \PY{n}{test\PYZus{}scores\PYZus{}std}\PY{p}{,}
                     \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{+} \PY{n}{test\PYZus{}scores\PYZus{}std}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}2492ff}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores\PYZus{}mean}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}ff9124}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{test\PYZus{}scores\PYZus{}mean}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}2492ff}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cross\PYZhy{}validation score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{title2}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training size (m)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{best}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Third Estimator}
    \PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{test\PYZus{}scores} \PY{o}{=} \PY{n}{learning\PYZus{}curve}\PY{p}{(}
        \PY{n}{estimator3}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{cv}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f1\PYZus{}weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{,} \PY{n}{train\PYZus{}sizes}\PY{o}{=}\PY{n}{train\PYZus{}sizes}\PY{p}{)}
    \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{train\PYZus{}scores\PYZus{}std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{train\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{test\PYZus{}scores\PYZus{}std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{\PYZhy{}} \PY{n}{train\PYZus{}scores\PYZus{}std}\PY{p}{,}
                     \PY{n}{train\PYZus{}scores\PYZus{}mean} \PY{o}{+} \PY{n}{train\PYZus{}scores\PYZus{}std}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                     \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}ff9124}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{\PYZhy{}} \PY{n}{test\PYZus{}scores\PYZus{}std}\PY{p}{,}
                     \PY{n}{test\PYZus{}scores\PYZus{}mean} \PY{o}{+} \PY{n}{test\PYZus{}scores\PYZus{}std}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}2492ff}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{train\PYZus{}scores\PYZus{}mean}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}ff9124}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}sizes}\PY{p}{,} \PY{n}{test\PYZus{}scores\PYZus{}mean}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}2492ff}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cross\PYZhy{}validation score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{title3}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training size (m)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{ax3}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{best}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
    \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}     return plt}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{estimator1} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}

\PY{n}{estimator2} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}

\PY{n}{estimator3} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} 
                                    \PY{n}{max\PYZus{}features} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                    \PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{40}\PY{p}{,} 
                                    \PY{n}{min\PYZus{}samples\PYZus{}split} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} 
                                    \PY{n}{min\PYZus{}samples\PYZus{}leaf} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}
                                    \PY{n}{bootstrap} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,}
                                    \PY{n}{criterion} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                    \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plot\PYZus{}learning\PYZus{}curve}\PY{p}{(}\PY{n}{estimator1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original dataset }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{RBF learning Curve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                    \PY{n}{estimator2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original dataset }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{KNN Learning Curve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                    \PY{n}{estimator3}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original dataset }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{RandomForest Learning Curve}\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} 
                    \PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.01}\PY{p}{)}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{cv\PYZus{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_107_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{107}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plot\PYZus{}learning\PYZus{}curve}\PY{p}{(}\PY{n}{estimator1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Outliers\PYZhy{}free }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{RBF Learning Curve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                    \PY{n}{estimator2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Outliers\PYZhy{}free }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{KNN Learning Curve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                    \PY{n}{estimator3}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Outliers\PYZhy{}free }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{RandomForest Learning Curve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                    \PY{n}{train\PYZus{}noisefree}\PY{p}{,} \PY{n}{trainlabels\PYZus{}noisefree}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.01}\PY{p}{)}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{cv\PYZus{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_108_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For both Original and Outliers-free splits

\begin{itemize}
\item
  RBF shows almost ideal learning case
\item
  KNN shows bias(underfitting): In the beginning with a small sample
  size the model is supposed to achieve a very low training error.
  However, it fails to do so, i.e.~it is not even able to learn/overfit
  the small training sample. Since increasing model complexity didn't
  help, KNN is underperformed
\item
  Random forests : Training score is at its maximum regardless of
  training examples and Cross-validation score increases over time and
  gap between cross-validation score and training score indicates high
  variance scenario (sign of overfitting)
\end{itemize}

Random forests are built on decision trees, and decision trees are
sensitive to class imbalance. Each tree is built on a bag, and each bag
is a uniform random sample from the data (with replacement). Therefore
each tree will be biased in the same direction and magnitude (on
average) by class imbalance.

    \hypertarget{imbalanced-dataset-classification-by-modifying-classifiers-cost-function}{%
\subsubsection{Imbalanced dataset classification by modifying
classifiers' cost
function}\label{imbalanced-dataset-classification-by-modifying-classifiers-cost-function}}

    By considering only original and Outlier-free datasets and best
performed classifiers with best performed hyperparameters, the same
classification implemented on modified classifiers' cost function by
penalizing mistakes on the minority class using
\textbf{class\_weight=`balanced'} in the SVC and RF while in KNN,
\textbf{weight=`distance'} weights points by the inverse of their
distance. in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{candidate\PYZus{}datasets} \PY{o}{=} \PY{p}{\PYZob{}}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original dataset}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{n}{train\PYZus{}data}\PY{p}{,}\PY{n}{train\PYZus{}labels}\PY{p}{,}\PY{n}{test\PYZus{}data}\PY{p}{,}\PY{n}{test\PYZus{}labels}\PY{p}{]}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{noise\PYZhy{}free}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{n}{train\PYZus{}noisefree}\PY{p}{,} \PY{n}{trainlabels\PYZus{}noisefree}\PY{p}{,} \PY{n}{test\PYZus{}noisefree}\PY{p}{,} \PY{n}{testlabels\PYZus{}noisefree}\PY{p}{]}
               \PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{69}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{candidate\PYZus{}classifiers} \PY{o}{=} \PY{p}{\PYZob{}}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Support Vector Classifier}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{SVC}\PY{p}{(}\PY{p}{)}\PY{p}{,}         \PY{c+c1}{\PYZsh{}  (probability=False) is compatible with imbalanced dataset}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{KNearest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,}       \PY{c+c1}{\PYZsh{}3 ok, 5 overfittting}
                        \PY{p}{\PYZcb{}}    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{87}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{candidate\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{svc\PYZus{}params}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{knears\PYZus{}params}\PY{p}{,}
             \PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{71}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}introducing cost function modifier parameters }
\PY{n}{pen\PYZus{}svc\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} 
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class\PYZus{}weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{probability}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{k+kc}{True}\PY{p}{]}\PY{p}{\PYZcb{}}

\PY{n}{pen\PYZus{}knears\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,} 
                 \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weights}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{distance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}

\PY{n}{pen\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{pen\PYZus{}svc\PYZus{}params}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{pen\PYZus{}knears\PYZus{}params}\PY{p}{,}
             \PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{d}\PY{o}{=}\PY{n}{imbalanced\PYZus{}clf}\PY{p}{(}\PY{n}{cv}\PY{o}{=}\PY{n}{cv\PYZus{}}\PY{p}{,}\PY{n}{ds}\PY{o}{=}\PY{n}{candidate\PYZus{}datasets}\PY{p}{,}\PY{n}{clf}\PY{o}{=}\PY{n}{candidate\PYZus{}classifiers}\PY{p}{,}\PY{n}{par}\PY{o}{=}\PY{n}{pen\PYZus{}params}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{column\PYZus{}names}\PY{o}{=}\PY{p}{[} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dataset split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{run\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 CV score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precision score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best parameters}\PY{l+s+s1}{\PYZsq{}}
             \PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{853}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{d}\PY{o}{.}\PY{n}{reindex}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{column\PYZus{}names}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{853}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
      Dataset split                 model run\_time  F1 CV score  \textbackslash{}
1  Original dataset  KNeighborsClassifier      0.0         0.80
0  Original dataset                   SVC     0.01         0.81
2        noise-free                   SVC     0.01         0.83
3        noise-free  KNeighborsClassifier      0.0         0.79

   F1 score on test set  Precision score on test set  \textbackslash{}
1                0.7992                       0.8058
0                0.8202                       0.8317
2                0.8394                       0.8430
3                0.8732                       0.8751

   Recall score on test set  \textbackslash{}
1                    0.7980
0                    0.8182
2                    0.8438
3                    0.8750

best parameters
1                                                     \{'n\_neighbors': 3,
'weights': 'distance'\}
0  \{'C': 10.0, 'class\_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf',
'probability': True\}
2  \{'C': 10.0, 'class\_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf',
'probability': True\}
3                                                     \{'n\_neighbors': 3,
'weights': 'distance'\}
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{858}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{estimator1} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{probability}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{estimator2} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{estimator3} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{distance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plot\PYZus{}learning\PYZus{}curve}\PY{p}{(}\PY{n}{estimator1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RBF Penalized}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{estimator2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{KNN}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{estimator3}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{KNN Penalized}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                    \PY{n}{train\PYZus{}noisefree}\PY{p}{,} \PY{n}{trainlabels\PYZus{}noisefree}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.01}\PY{p}{)}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{cv\PYZus{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_119_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    by modifying the \textbf{costs of misclassification} the performance
increased for both dataset splits

\textbf{from the learning curve it is very clear how by modifying the
cost function of KNN, the underfitting problem is solved}

    \hypertarget{dataset-balancing}{%
\section{Dataset Balancing:}\label{dataset-balancing}}

Since the size of the dataset is relatively small, only
\textbf{\emph{oversampling}} of the minority classes using two of the
most common technique will be considered

\hypertarget{synthetic-minority-oversampling-technique-smote}{%
\subsection{Synthetic Minority Oversampling Technique
(SMOTE)}\label{synthetic-minority-oversampling-technique-smote}}

Considering a sample \(x_i\), a new sample \(x_{new}\) will be generated
considering its \(k\) neareast-neighbors. Then, one of these
nearest-neighbors \(x_{zi}\) is selected and a sample is generated as
follows:{[}x\_\{new\} = x\_i + \lambda \times (x\_\{zi\} - x\_i){]}
where \(\lambda\) is a random number in the range {[}0, 1{]}. This
interpolation will create a sample on the line between \(x_{i}\) and
\(x_{zi}\)

\hypertarget{adaptive-synthetic-adasyn}{%
\subsection{Adaptive Synthetic
(ADASYN)}\label{adaptive-synthetic-adasyn}}

ADASYN uses the same algorithms to generate samples, the key difference
is that ADASYN uses a \textbf{density distribution}, as a criterion to
automatically decide the number of synthetic samples that must be
generated for each minority sample by adaptively changing the weights of
the different minority samples to compensate for the skewed
distributions. SMOTE generates the same number of synthetic samples for
each original minority sample.

    For multiple classes classification, both ADASYN and SMOTE need
information regarding the neighbourhood of each sample used for sample
generation. They are using a \textbf{one-vs-rest} approach by selecting
each targeted class and computing the necessary statistics against the
rest of the data set which are grouped in a single class.

\textbf{imbalanced-learn} is a python package offering a number of
re-sampling techniques commonly used in datasets showing strong
between-class imbalance. It is compatible with scikit-learn.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{96}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} conda install \PYZhy{}c glemaitre imbalanced\PYZhy{}learn}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import imblearn}
\PY{k+kn}{from} \PY{n+nn}{imblearn}\PY{n+nn}{.}\PY{n+nn}{over\PYZus{}sampling} \PY{k+kn}{import} \PY{n}{SMOTE} 
\PY{n}{X} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language\PYZus{}coded}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{Y} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{language\PYZus{}coded}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{sm} \PY{o}{=} \PY{n}{SMOTE}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\PY{n}{X\PYZus{}res}\PY{p}{,} \PY{n}{Y\PYZus{}res} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{fit\PYZus{}resample}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{)}
\PY{c+c1}{\PYZsh{} df\PYZus{}balanced = pd.concat([pd.DataFrame(X\PYZus{}res), pd.DataFrame(Y\PYZus{}res, columns=[\PYZsq{}language\PYZus{}coded\PYZsq{}])], axis=1) \PYZsh{}data split\PYZsh{}4}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SMOTE over\PYZhy{}sampling:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{Y\PYZus{}res}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{Y\PYZus{}res}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count (target)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
SMOTE over-sampling:
5    165
4    165
3    165
2    165
1    165
0    165
Name: language\_coded, dtype: int64
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_124_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{77}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}plot }
\PY{k+kn}{from} \PY{n+nn}{imblearn}\PY{n+nn}{.}\PY{n+nn}{over\PYZus{}sampling} \PY{k+kn}{import} \PY{n}{ADASYN}
\PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k+kn}{import} \PY{n}{where}

\PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} scatter plot of examples by class label}
\PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k+kn}{import} \PY{n}{array}
\PY{n}{g} \PY{o}{=} \PY{n}{array}\PY{p}{(}\PY{n}{predictors}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{k} \PY{o}{=} \PY{n}{language}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)} 
\PY{n}{k}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{o}{.}\PY{n}{T}

\PY{k+kn}{import} \PY{n+nn}{itertools}
\PY{n}{colors} \PY{o}{=} \PY{n}{itertools}\PY{o}{.}\PY{n}{cycle}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tab:blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tab:brown}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tab:green}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tab:purple}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tab:red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tab:orange}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{colors\PYZus{}} \PY{o}{=} \PY{n}{itertools}\PY{o}{.}\PY{n}{cycle}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tab:brown}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tab:orange}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}

\PY{k+kn}{from} \PY{n+nn}{collections} \PY{k+kn}{import} \PY{n}{Counter}
\PY{n}{counter}\PY{o}{=}\PY{n}{Counter}\PY{p}{(}\PY{n}{language}\PY{p}{)}
\PY{k}{for} \PY{n}{label}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{counter}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{row\PYZus{}ix} \PY{o}{=} \PY{n}{where}\PY{p}{(}\PY{n}{k} \PY{o}{==} \PY{n}{label}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{g}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n+nb}{str}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}X1 ,X10 choosen from orange \PYZsq{}find informative projections\PYZsq{}}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k}{for} \PY{n}{label} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{US}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}    
    \PY{n}{row\PYZus{}ix} \PY{o}{=} \PY{n}{where}\PY{p}{(}\PY{n}{k} \PY{o}{==} \PY{n}{label}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{g}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n+nb}{str}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors\PYZus{}}\PY{p}{)}\PY{p}{)}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{US vs Original FR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{oversample} \PY{o}{=} \PY{n}{SMOTE}\PY{p}{(}\PY{p}{)}
\PY{n}{g\PYZus{}smote}\PY{p}{,} \PY{n}{k\PYZus{}smote} \PY{o}{=} \PY{n}{oversample}\PY{o}{.}\PY{n}{fit\PYZus{}resample}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{k}\PY{p}{)}

\PY{c+c1}{\PYZsh{} for label, \PYZus{} in counter.items():}
\PY{k}{for} \PY{n}{label} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{US}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}    
    \PY{n}{row\PYZus{}ix} \PY{o}{=} \PY{n}{where}\PY{p}{(}\PY{n}{k\PYZus{}smote} \PY{o}{==} \PY{n}{label}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{g\PYZus{}smote}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{g\PYZus{}smote}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n+nb}{str}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors\PYZus{}}\PY{p}{)}\PY{p}{)}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{US vs SMOTE FR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    
\PY{k}{for} \PY{n}{label}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{counter}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{row\PYZus{}ix} \PY{o}{=} \PY{n}{where}\PY{p}{(}\PY{n}{k\PYZus{}smote} \PY{o}{==} \PY{n}{label}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{g\PYZus{}smote}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{g\PYZus{}smote}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n+nb}{str}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}X1 ,X10 choosen from orange \PYZsq{}find informative projections\PYZsq{}}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SMOTE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{ada} \PY{o}{=} \PY{n}{ADASYN}\PY{p}{(}\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{101}\PY{p}{)}
\PY{n}{g\PYZus{}adasyn}\PY{p}{,} \PY{n}{k\PYZus{}adasyn} \PY{o}{=} \PY{n}{ada}\PY{o}{.}\PY{n}{fit\PYZus{}resample}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{k}\PY{p}{)}

\PY{k}{for} \PY{n}{label} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{US}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}    
    \PY{n}{row\PYZus{}ix} \PY{o}{=} \PY{n}{where}\PY{p}{(}\PY{n}{k\PYZus{}adasyn} \PY{o}{==} \PY{n}{label}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{g\PYZus{}adasyn}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{g\PYZus{}adasyn}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n+nb}{str}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors\PYZus{}}\PY{p}{)}\PY{p}{)}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{US vs ADASYN FR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    
\PY{k}{for} \PY{n}{label}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{counter}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{row\PYZus{}ix} \PY{o}{=} \PY{n}{where}\PY{p}{(}\PY{n}{k\PYZus{}adasyn} \PY{o}{==} \PY{n}{label}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{g\PYZus{}adasyn}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{g\PYZus{}adasyn}\PY{p}{[}\PY{n}{row\PYZus{}ix}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n+nb}{str}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}X1 ,X10 choosen from orange \PYZsq{}find informative projections\PYZsq{}}
    \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ADA}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_125_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{oversampled-dataset-classification}{%
\subsection{Oversampled Dataset
Classification}\label{oversampled-dataset-classification}}

    Cross-Validaion for oversampled dataset is tricky and need a bit of work
on coding mainly \textbf{to avoid validating a training fold on
synthetic data} as shown on the example below \textbf{synthetic data
should only be used for training}, and neiher for validation nor for
testing

    

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{78}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{imblearn}\PY{n+nn}{.}\PY{n+nn}{pipeline} \PY{k+kn}{import} \PY{n}{make\PYZus{}pipeline} \PY{k}{as} \PY{n}{imbalanced\PYZus{}make\PYZus{}pipeline}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{79}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cv\PYZus{}} \PY{o}{=} \PY{n}{StratifiedKFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{n}{splits}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{88}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bestclfparams\PYZus{}oversamp} \PY{p}{(}\PY{n}{mode}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{cv\PYZus{}}\PY{p}{,}\PY{n}{datasets}\PY{o}{=}\PY{n}{candidate\PYZus{}datasets}\PY{p}{,}\PY{n}{clf}\PY{o}{=}\PY{n}{candidate\PYZus{}classifiers}\PY{p}{,}\PY{n}{par}\PY{o}{=}\PY{n}{candidate\PYZus{}params}\PY{p}{)}\PY{p}{:}
    
    \PY{n}{df\PYZus{}results} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}

    \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{datasets}\PY{p}{:}
        \PY{n}{train\PYZus{}pred}\PY{o}{=}\PY{n}{datasets}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{train\PYZus{}y}\PY{o}{=}\PY{n}{datasets}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{test\PYZus{}pred}\PY{o}{=}\PY{n}{datasets}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
        \PY{n}{test\PYZus{}y}\PY{o}{=}\PY{n}{datasets}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}

        \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{train\PYZus{}pred}\PY{o}{.}\PY{n}{values}
        \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{test\PYZus{}pred}\PY{o}{.}\PY{n}{values}
        \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{train\PYZus{}y}\PY{o}{.}\PY{n}{values}
        \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{test\PYZus{}y}\PY{o}{.}\PY{n}{values}
        
        \PY{n}{accuracy\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{precision\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{recall\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{f1\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{auc\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}

\PY{c+c1}{\PYZsh{}         for key,classifier,paramsname,params in zip(classifiers.items(),params.items()):}
        \PY{k}{for} \PY{p}{(}\PY{n}{key1}\PY{p}{,}\PY{n}{classifier}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{key2}\PY{p}{,}\PY{n}{parameter}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{par}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} grid=GridSearchCV(classifier, parameter)}
            \PY{n}{start\PYZus{}time} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}

            \PY{n}{grid}\PY{o}{=}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{classifier}\PY{p}{,} \PY{n}{parameter}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{cv}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f1\PYZus{}macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

            \PY{k}{for} \PY{n}{train}\PY{p}{,} \PY{n}{test} \PY{o+ow}{in} \PY{n}{cv}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{:}
                \PY{k}{if} \PY{n}{mode}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                    \PY{n}{pipeline} \PY{o}{=} \PY{n}{imbalanced\PYZus{}make\PYZus{}pipeline}\PY{p}{(}\PY{n}{SMOTE}\PY{p}{(}\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}\PY{n}{sampling\PYZus{}strategy}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{minority}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{grid}\PY{p}{)}
                \PY{k}{if} \PY{n}{mode}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ada}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} 
                    \PY{n}{pipeline} \PY{o}{=} \PY{n}{imbalanced\PYZus{}make\PYZus{}pipeline}\PY{p}{(}\PY{n}{ADASYN}\PY{p}{(}\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{)}\PY{p}{,} \PY{n}{grid}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} pipeline = Pipeline([(\PYZsq{}sm\PYZsq{}, SMOTE(sampling\PYZus{}strategy=\PYZsq{}minority\PYZsq{})), (\PYZsq{}clf\PYZsq{}, grid)])}
                \PY{n}{model} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{)}
                \PY{n}{best\PYZus{}est} \PY{o}{=} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
                \PY{n}{prediction} \PY{o}{=} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{test}\PY{p}{]}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} print(best\PYZus{}est)}
                \PY{n}{f1\PYZus{}lst}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{test}\PY{p}{]}\PY{p}{,} \PY{n}{prediction}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}weighted is the best calculated on KNN+6folds}

            \PY{n}{row} \PY{o}{=} \PY{p}{\PYZob{}}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dataset split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{key}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{classifier}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}class\PYZus{}\PYZus{}}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{run\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start\PYZus{}time}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{60}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 CV score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{f1\PYZus{}lst}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Acc score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pr score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rec score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{round}\PY{p}{(}\PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}est}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weighted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best parameters}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{,}
                   \PY{p}{\PYZcb{}}
            \PY{n}{df\PYZus{}results} \PY{o}{=} \PY{n}{df\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{row}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
    \PY{k}{return} \PY{n}{df\PYZus{}results}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{81}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{column\PYZus{}names}\PY{o}{=}\PY{p}{[} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dataset split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{run\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 CV score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Acc score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pr score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rec score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best parameters}\PY{l+s+s1}{\PYZsq{}}
             \PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{82}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{k5}\PY{o}{=}\PY{n}{bestclfparams\PYZus{}oversamp} \PY{p}{(}\PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{83}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{k5}\PY{o}{.}\PY{n}{reindex}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{column\PYZus{}names}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{83}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
      Dataset split                 model run\_time  F1 CV score  \textbackslash{}
1  Original dataset  KNeighborsClassifier     0.01         79.9
0  Original dataset                   SVC     15.2         83.2
3        noise-free  KNeighborsClassifier     0.01         75.3
2        noise-free                   SVC    13.88         80.0

   F1 score on test set  Acc score on test set  pr score on test set  \textbackslash{}
1                 79.11                  78.79                 81.02
0                 81.32                  81.82                 82.07
3                 85.43                  85.42                 86.71
2                 86.18                  86.46                 86.32

   rec score on test set                              best parameters
1                  78.79                           \{'n\_neighbors': 3\}
0                  81.82  \{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'\}
3                  85.42                           \{'n\_neighbors': 3\}
2                  86.46  \{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'\}
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{89}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{l}\PY{o}{=}\PY{n}{bestclfparams\PYZus{}oversamp} \PY{p}{(}\PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ada}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{90}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{l}\PY{o}{.}\PY{n}{reindex}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{column\PYZus{}names}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1 score on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{90}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
      Dataset split                 model run\_time  F1 CV score  \textbackslash{}
1  Original dataset  KNeighborsClassifier     0.02         79.3
0  Original dataset                   SVC     0.22         83.2
3        noise-free  KNeighborsClassifier     0.02         76.5
2        noise-free                   SVC     0.18         80.1

   F1 score on test set  Acc score on test set  pr score on test set  \textbackslash{}
1                 76.84                  76.77                 83.02
0                 81.07                  80.81                 82.98
3                 81.75                  81.25                 84.98
2                 83.88                  84.38                 83.62

   rec score on test set                              best parameters
1                  76.77                           \{'n\_neighbors': 3\}
0                  80.81  \{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'\}
3                  81.25                           \{'n\_neighbors': 3\}
2                  84.38  \{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'\}
\end{Verbatim}
\end{tcolorbox}
        
    \begin{itemize}
\tightlist
\item
  SMOTE increased the performance of the models on the Outliers-free
  dataset only
\item
  ADASYN increased only SVC for the outliers-free split
\end{itemize}

    \begin{longtable}[]{@{}llll@{}}
\toprule
Dataset split & Model & Method & F1-score\tabularnewline
\midrule
\endhead
Outlier-free & KNN & SMOTE & 0.854\tabularnewline
Outlier-free & RBF & SMOTE & 0.862\tabularnewline
Outlier-free & KNN & Penalized & 0.873\tabularnewline
\bottomrule
\end{longtable}

    Above is the top three performed models

    \hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

A sample of 329 signals, containing 165 US voice and 164 non-US voice(45
UK, 29 ES, and 30 for IT,GR and FR each), is analyzed for the purpose of
accent recognition. For each signal, the mean vector of MFCC matrix is
used as an input vector for pattern recognition. Then different
classifiers are compared based on the MFCC feature. By comparison,
Support Vector Machine (RBF) and K-nearest Kneighbour yield the highest
average test F1-Score, after using 10-folds cross-validation.

by minimizing the influence of the majority class by giving the closest
neighbours more weight, KNN leverages the score about 5\%. By performing
SMOTE and ADASYN on the minority classes during cross-validation, we
managed to elevate both KNN and RBF F1-score abount 4\%

Cleaning the minimum outlier/extreme data proved to help increasing
classifiers performance regardless whether these instances are actually
noise or not

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
